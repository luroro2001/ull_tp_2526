# Exercise 2: Barnes Hut N-body simulation

This exercise consists in the implementation of an N-body gravitational simulation using the Barnes-Hut algorithm to reduce computational complexity from $\mathcal{O}(N^2)$ to approximately $\mathcal{O}(N \log N)$.

Three versions of the code are provided:

- Serial implementation  
- Shared-memory parallel version using OpenMP 
- Distributed-memory parallel version using MPI


## Project structure
```
ull_tp_2526/students/rl/course_exercise_2/
│
├── src/
│ ├── geometry.f90 # 3D geometry types and vector operations
│ ├── particle.f90 # Definition of particle3d type
│ └── bh.f90 # Barnes–Hut algorithm implementation
│
├── openmp/
│ └── main.f90 # main program for Serial/OpenMP
│
├── mpi/
│ └── main_mpi.f90 # main program for MPI
│
├── timings.txt # Execution times obtained for each implementation 
├── perfomance_plot.py # Plots performance data from timings.txt
├── sphere_generator.f90 # Generator of random initial conditions
├── input.dat # Input file for simulations (generated by sphere_generator)
├── Makefile 
└── README.md 
```

## Code explanation

The core calculations and data structures (`particle.f90`, `geometry.f90`, `bh.f90`) are located in `src/` and are shared by all implementations. Parallelism is introduced only at the level of the main programs, `main.f90` and `main_mpi.f90`.

### **`particle.f90`**

Defines a module 'particle' that contains a type 'particle3d' to represent a particle in 3D space. 
Said type has the following components:
- a point3d variable 'p' storing the particle's position
- a vector3d variable 'v' storing the particle's velocity
- a real variable 'm' storing the particle's mass  

It is used to store and manipulate particles during integration.

### **`geometry.f90`**

Defines the basic geometric operations that are used throughout the simulation.

Contains:
- data types vector3d and point 3d
- functions sumvp, sumpv, sumvv, sumpp, subvp, subpv, subpp, mulrv, mulvr, divvr
- operators matching these functions
- additional functions: distance, angle, norm, normalize, cross_product, dot_prod


### **`bh.f90`**

Implements the Barnes-Hut octree algorithm for gravitational force computation.

Main responsibilities:
- Construction of the octree (`cell` type)
- Recursive subdivision of space
- Computation of centers of mass
- Force evaluation

Key features:
- Each cell can be:
    - empty
    - contain a sngle particle
    - represent a group of particles
- The force computation is parallelized with OpenMp in `calculate_forces` and partitioned across processes in `calculate_forces_mpi`.

This module is shared by all three implementations.

### **`sphere_generator.f90`**

Generates random initial conditions for the simulation. 
- Particles are uniformly distributed inside a unit sphere
- Velocities are initially set to zero
- Mass is uniformly distributed
- Outputs a valid `input.dat` file format

Said input format is:

```
dt
dt_out
t_end
n
m1 x1 y1 z1 vx1 vy1 vz1
m2 x2 y2 z2 vx2 vy2 vz2
...
mn xn yn zn vxn vyn vzn
```

Example usage: 

```
gfortran -O2 -o generate_input sphere_generator.f90
echo N | ./generate_input > input.dat # N is the number of particles chosen by the user
```

### **`main.f90`**

1. Reads simulation parameters and particle data from `input.dat`

2. Builds the initial Brnes-Hut tree

3. Computes initial accelerations

4. Advances the system in time

5. Writes particle positions to `output.dat`

6. Measures total execution time using `omp_get_wtime`

### **`main_mpi.f90`**

Implements a distributed-memory version using MPI.

**PENDING**: complete explanation once I've looked into why it doesn`t improve performance.

## Tutorial

1. Prerequisites

You must have installed:
- `gfortran`
- `OpenMP` support
- `MPI` implementation

2. Generate an input file
```
gfortran -O2 -o generate_input sphere_generator.f90
echo N | ./generate_input > input.dat # N is the number of particles chosen by the user
```
3.  Compile the code and run the simulation

These commands must be ran from the main directory (course_exercise_2).

**Serial**

```
make
./nbody_bh < input.dat
```

**OpenMP**

```
make nbody_bh_omp
export OMP_NUM_THREADS=4    # to run with 4 threads
./nbody_bh_omp < input.dat
```

**MPI**
```
make mpi
mpirun -np 4 ./nbody_bh_mpi < input.dat # to run with 4 processes
```

## Performance anaylisis

Let's now compare the performance of serial, OpenMP, and MPI implementations for different numbers of particles. The results obtained (which can also be found in timings.txt) for particle counts ranging from N=10 to N=5000 are summarized in the figure below.

<img width="1200" height="900" alt="time_plot" src="https://github.com/user-attachments/assets/84da2598-fa41-4da4-b4bc-30bdf493fc89" />


For N=10, the serial version seems to perform best. Bith OpenMP and MPI show a worse performance, possibly because the overhead associated with the thread creation, synchronization, and MPI communication dominates the actual computation. In this regime, parallelization is seemingly not beneficial.

For slightly larger counts, such as N=100, the performance of all versions becomes comparable. OpenMP with multiple threads and MPI with multiple processes begin to reduce execution time, but the gains remain modest, indicating that parallel overhead is still significant relative to the computational workload.

As N increases to 500 and 1000, clear performance trends emerge. The serial execution time grows, while the MPI version with multiple processes consistently outperforms both the serial and OpenMP implementations. This reflects the higher scalability of the distributed-memory approach, where the force computation is effectively partitioned across processes. OpenMP shows some speedup when increasing the number of threads, but its performance is generally limited in comparison.

For the largest case, N=5000, MPI with 2 processes achieves the best performance overall, significantly reducing execution time compared to both the serial and OpenMP versions. MPI with 4 processes does not improve further and even degrades performance, maybe suggesting that communication and synchronization costs begin to outweigh the benefits of additional parallelism for this particle count and implementation. OpenMP apparently exhibits limited scalability at large N, with execution times remaining close to or worse than the serial case.
