# Exercise 2: Barnes Hut N-body simulation

This exercise consists in the implementation of an N-body gravitational simulation using the Barnes-Hut algorithm to reduce computational complexity from \(\mathcal{O}(N^2)\) to approximately \(\mathcal{O}(N \log N)\).

Three versions of the code are provided:

- **Serial** implementation  
- **Shared-memory parallel** version using **OpenMP**  
- **Distributed-memory parallel** version using **MPI**


## Project structure
```
ull_tp_2526/students/rl/course_exercise_2/
│
├── src/
│ ├── geometry.f90 # 3D geometry types and vector operations
│ ├── particle.f90 # Definition of particle3d type
│ └── bh.f90 # Barnes–Hut algorithm implementation
│
├── openmp/
│ └── main.f90 # main program for Serial/OpenMP
│
├── mpi/
│ └── main_mpi.f90 # main program for MPI
│
├── sphere_generator.f90 # Generator of random initial conditions
├── input.dat # Input file for simulations (generated by sphere_generator)
├── Makefile 
└── README.md 
```

## Code explanation

The core calculations and data structures (`particle.f90`, `geometry.f90`, `bh.f90`) are located in `src/` and are shared by all implementations. Parallelism is introduced only at the level of the main programs, `main.f90` and `main_mpi.f90`.

### **`particle.f90`**

Defines a module 'particle' that contains a type 'particle3d' to represent a particle in 3D space. 
Said type has the following components:
- a point3d variable 'p' storing the particle's position
- a vector3d variable 'v' storing the particle's velocity
- a real variable 'm' storing the particle's mass  

It is used to store and manipulate particles during integration.

### **`geometry.f90`**

Defines the basic geometric operations that are used throughout the simulation.

Contains:
- data types vector3d and point 3d
- functions sumvp, sumpv, sumvv, sumpp, subvp, subpv, subpp, mulrv, mulvr, divvr
- operators matching these functions
- additional functions: distance, angle, norm, normalize, cross_product, dot_prod


### **`bh.f90`**

Implements the Barnes-Hut octree algorithm for gravitational force computation.

Main responsibilities:
- Construction of the octree (`cell` type)
- Recursive subdivision of space
- Computation of centers of mass
- Force evaluation

Key features:
- Each cell can be:
    - empty
    - contain a sngle particle
    - represent a group of particles
- The force computation is parallelized with OpenMp in `calculate_forces` and partitioned across processes in `calculate_forces_mpi`.

This module is shared by all three implementations.

### **`sphere_generator.f90`**

Generates random initial conditions for the simulation. 
- Particles are uniformly distributed inside a unit sphere
- Velocities are initially set to zero
- Mass is uniformly distributed
- Outputs a valid `input.dat` file format

Said input format is:

```
dt
dt_out
t_end
n
m1 x1 y1 z1 vx1 vy1 vz1
m2 x2 y2 z2 vx2 vy2 vz2
...
mn xn yn zn vxn vyn vzn
```

Example usage: 

```
gfortran -O2 -o generate_input sphere_generator.f90
echo N | ./generate_input > input.dat # N is the number of particles chosen by the user
```

### **`main.f90`**

1. Reads simulation parameters and particle data from `input.dat`

2. Builds the initial Brnes-Hut tree

3. Computes initial accelerations

4. Advances the system in time

5. Writes particle positions to `output.dat`

6. Measures total execution time using `omp_get_wtime`

### **`main_mpi.f90`**

Implements a distributed-memory version using MPI.

**PENDING**: complete explanation once I've looked into why it doesn`t improve performance.

## Tutorial

1. Prerequisites.

You must have installed:
- `gfortran`
- `OpenMP` support
- `MPI` implementation

2. Generate an input file
```
gfortran -O2 -o generate_input sphere_generator.f90
echo N | ./generate_input > input.dat # N is the number of particles chosen by the user
```
3.  Compile the code and run the simulation

These commands must be ran from the main directory (course_exercise_2).

**Serial**

```
make
./nbody_bh < input.dat
```

**OpenMP**

```
make nbody_bh_omp
export OMP_NUM_THREADS=4    # to run with 4 threads
./nbody_bh_omp < input.dat
```

**MPI**
```
make mpi
mpirun -np 4 ./nbody_bh_mpi < input.dat # to run with 4 processes
```

## Performance anaylisis

Let's now compare the performance of serial, OpenMP, and MPI implementations for different numbers of particles.

For N=10:
Serial: 0.24884013199971378
OpenMP: 0.25683108600060223 
MPI: 0.16862512499999999

For N=100:
Serial: 1.4958445229995050
OpenMP (4 threads): 1.5268324380003833 
MPI: 1.1175616639999999 

For N=500:
Serial: 8.1878654459997051 
OpenMP: 7.7628420060000281
MPI (4 processes): 6.1154894380000000

For N=1000:
Serial: 27.422356600000057
OpenMP: 29.105342273998758 
MPI: 27.239613327000001

**PENDING**: running more cases with different thread counts and N. Possibly plotting them.